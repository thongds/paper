{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Fy0nt3CF9q_N"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (0.26.2)\n",
            "Requirement already satisfied: tensorflow in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (2.20.0)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (3.10.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from tensorflow) (2.3.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google_pasta>=0.1.1 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from tensorflow) (6.31.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from tensorflow) (78.1.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from tensorflow) (2.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.20.0 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from tensorflow) (2.20.0)\n",
            "Requirement already satisfied: keras>=3.10.0 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from tensorflow) (3.12.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from tensorboard~=2.20.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: pillow in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from tensorboard~=2.20.0->tensorflow) (11.2.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
            "Requirement already satisfied: namex in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/rl_env/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym tensorflow numpy matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VonZ9m5g9w4_"
      },
      "source": [
        "Import all relevant libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pHXmu5cm9tSq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/7y/tb8vlfv937523rgqdlf0zjy40000gn/T/ipykernel_55844/3540414002.py:9: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not hasattr(np, 'bool8'):\n",
            "/opt/anaconda3/envs/rl_env/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.0 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/envs/rl_env/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.0 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/envs/rl_env/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.0 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/envs/rl_env/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.0 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/envs/rl_env/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.0 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/envs/rl_env/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.0 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/envs/rl_env/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.0 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/envs/rl_env/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.0 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/envs/rl_env/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.0 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/envs/rl_env/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.0 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/envs/rl_env/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.0 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/envs/rl_env/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.0 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/envs/rl_env/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.0 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/envs/rl_env/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.0 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/envs/rl_env/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.0 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/envs/rl_env/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.0 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/envs/rl_env/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.0 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/envs/rl_env/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.0 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/envs/rl_env/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.0 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/envs/rl_env/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.0 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import gym\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import highway_env\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if not hasattr(np, 'bool8'):\n",
        "    np.bool8 = np.bool_\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, optimizers\n",
        "from pyvirtualdisplay import Display\n",
        "from TransitionModelLearnerDQN import TransitionModelLearnerDQN\n",
        "\n",
        "#from gym.wrappers.monitoring import video_recorder\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S70_Kbke94Tp"
      },
      "source": [
        "Write up a DQN class that builds the model, build the replay buffer, implements epsilon-greedy exploration and updates the Q network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teV9aWxrlFm1"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = []\n",
        "        self.gamma = 0.9  # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.01 # minimum epsilon\n",
        "        self.epsilon_decay = 0.995 #rate of epsilon decay\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model()#build the model\n",
        "        self.transition_learner = TransitionModelLearnerDQN()\n",
        "        self.rng = np.random.default_rng(123)\n",
        "        \n",
        "    def _build_model(self):\n",
        "        #this function builds the MLP model with relu activations in 2 the hidden layers, and linear activation in the output layer\n",
        "        model = models.Sequential()\n",
        "        model.add(layers.Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(layers.Dense(24, activation='relu'))\n",
        "        model.add(layers.Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=optimizers.Adam(learning_rate=self.learning_rate))#specify the type of loss and the learning rate\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        #This function pushes instances of experience into the replay buffer\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state, action_list):\n",
        "        #epsilon-greedy exploration to select actions\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(action_list)\n",
        "        act_values = self.model.predict(state,verbose=0)\n",
        "        return np.argmax(act_values[0])\n",
        "    \n",
        "    def enhanced_epsilon_greedy(self,state, base_action, extend_action, epsilon, rng, encourage_new_action=False):\n",
        "        if encourage_new_action:\n",
        "            if rng.random() < 0.3:  \n",
        "                return np.random.choice(extend_action)\n",
        "        # Standard epsilon-greedy\n",
        "        if rng.random() < epsilon:\n",
        "                return np.random.choice(extend_action + base_action)\n",
        "\n",
        "        act_values = self.model.predict(state, verbose=0)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    \n",
        "    def replay(self, batch_size):\n",
        "        minibatch = np.random.choice(len(self.memory), batch_size, replace=False)#select a minibatch of size batch_size\n",
        "        states = np.array([self.memory[i][0].flatten() for i in minibatch])\n",
        "        actions = np.array([self.memory[i][1] for i in minibatch])\n",
        "        rewards = np.array([self.memory[i][2] for i in minibatch])\n",
        "        next_states = np.array([self.memory[i][3].flatten() for i in minibatch])\n",
        "        dones = np.array([self.memory[i][4] for i in minibatch])\n",
        "\n",
        "        targets = rewards + self.gamma * np.amax(self.model.predict_on_batch(next_states), axis=1) * (1 - dones)#form the learning targets\n",
        "        target_f = self.model.predict_on_batch(states)#current estimate of Q values\n",
        "        target_f[np.arange(batch_size), actions] = targets#update only the Q values corresponding to the action taken. Leave the others unchanged.\n",
        "\n",
        "        self.model.fit(states, target_f, epochs=1, verbose=0)#train the model\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)\n",
        "        \n",
        "    def train(self):\n",
        "        env = gym.make(\"highway-v0\", render_mode=\"rgb_array\")\n",
        "        # Handle observation space - flatten if needed\n",
        "        obs_space = env.observation_space\n",
        "        if hasattr(obs_space, 'shape'):\n",
        "            state_size = np.prod(obs_space.shape)\n",
        "        else:\n",
        "            state_size = obs_space.n\n",
        "        action_size = env.action_space.n\n",
        "        model_train_frequency = 5\n",
        "        batch_size = 32 #set batch size\n",
        "        EPISODES = 100 #Set total number of episodes you want the training to last\n",
        "        total_rewards_log=[]\n",
        "        base_action = [0, 2]  # Define base actions: left and right\n",
        "        expand_action = [1, 3, 4]\n",
        "        # e.g. {'LANE_LEFT': 0, 'IDLE': 1, 'LANE_RIGHT': 2, 'FASTER': 3, 'SLOWER': 4}\n",
        "        action_list = [0, 1, 2, 3, 4]\n",
        "        eps = self.epsilon\n",
        "        for ep in range(EPISODES):\n",
        "            state, info = env.reset()\n",
        "            state = np.reshape(state, [1, state_size])\n",
        "            total_reward=0\n",
        "            done = False\n",
        "            while done is False:#upto 200 steps in the episode\n",
        "                env.unwrapped.render()\n",
        "                \n",
        "                if ep < 30:  \n",
        "                    action = self.enhanced_epsilon_greedy(state, base_action, expand_action, eps, self.rng, encourage_new_action=True) # This part encourages exploration of new actions \n",
        "                else:\n",
        "                    action = self.enhanced_epsilon_greedy(state, base_action, expand_action, eps, self.rng, encourage_new_action=False)\n",
        "                \n",
        "                if action in expand_action and self.transition_learner.can_predict(): \n",
        "                    snext_model = self.transition_learner.predict_next_state(state, state_size, action, expand_action)\n",
        "                    q_values_next_state_model = self.model.predict(snext_model, verbose=0)\n",
        "                    q_value_current_state = self.model.predict(state, verbose=0)\n",
        "                    if np.max(q_values_next_state_model) > np.max(q_value_current_state):  \n",
        "                        reuse += 1\n",
        "                    else: \n",
        "                        reject += 1\n",
        "                        action = self.enhanced_epsilon_greedy(state, base_action, expand_action, eps, self.rng, encourage_new_action=False)\n",
        "                next_state, reward, done, truncated, info = env.step(action)#take the action, get reward and next state\n",
        "                \n",
        "                if action in expand_action:\n",
        "                    self.transition_learner.add_experience(state, next_state, action, expand_action)        \n",
        "                \n",
        "                done = done or truncated\n",
        "                total_reward+=reward# keep track of total rewards\n",
        "                next_state = np.reshape(next_state, [1, state_size])#reshape states to vectorised form\n",
        "                self.remember(state, action, reward, next_state, done)#store in replay buffer\n",
        "                state = next_state\n",
        "                \n",
        "                if ep > 0 and ep % model_train_frequency == 0 and len(self.transition_learner.buffer) > 50:\n",
        "                    self.transition_learner.train_model(batch_size=32, epochs=2)\n",
        "                \n",
        "                if len(self.memory) > batch_size:#if there are enough samples to form a batch then call experience replay\n",
        "                    self.replay(batch_size)#update the DQN weights through replay\n",
        "            total_rewards_log.append(total_reward)#log the total rewards in the episode\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-wXjRwl-V-F"
      },
      "source": [
        "Let's put it all together in the main loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORzf1620-Wjr"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/rl_env/lib/python3.10/site-packages/keras/src/layers/core/dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/opt/anaconda3/envs/rl_env/lib/python3.10/site-packages/keras/src/layers/core/dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (1x26 and 3x64)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m action_size \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n\u001b[1;32m      9\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQNAgent(state_size, action_size)\u001b[38;5;66;03m#initialise the agent\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[3], line 103\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menhanced_epsilon_greedy(state, base_action, expand_action, eps, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrng, encourage_new_action\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m expand_action \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransition_learner\u001b[38;5;241m.\u001b[39mcan_predict(): \n\u001b[0;32m--> 103\u001b[0m     snext_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransition_learner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_next_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     q_values_next_state_model \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(snext_model, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    105\u001b[0m     q_value_current_state \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(state, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
            "File \u001b[0;32m~/Documents/GitHub/AI/SIT723/game_env/TransitionModelLearnerDQN.py:94\u001b[0m, in \u001b[0;36mTransitionModelLearnerDQN.predict_next_state\u001b[0;34m(self, state, state_size, action, actions_list)\u001b[0m\n\u001b[1;32m     91\u001b[0m input_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(input_features, [\u001b[38;5;241m1\u001b[39m, state_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     92\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(input_features)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 94\u001b[0m predicted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m predicted_np \u001b[38;5;241m=\u001b[39m predicted\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predicted_np\n",
            "File \u001b[0;32m/opt/anaconda3/envs/rl_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/rl_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/Documents/GitHub/AI/SIT723/game_env/TransitionModelLearnerDQN.py:31\u001b[0m, in \u001b[0;36mTransitionModelLearnerDQN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through the neural network\"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     33\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n",
            "File \u001b[0;32m/opt/anaconda3/envs/rl_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/rl_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/rl_env/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x26 and 3x64)"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "env = gym.make(\"highway-v0\", render_mode=\"rgb_array\")\n",
        "# Handle observation space - flatten if needed\n",
        "obs_space = env.observation_space\n",
        "if hasattr(obs_space, 'shape'):\n",
        "    state_size = np.prod(obs_space.shape)\n",
        "else:\n",
        "    state_size = obs_space.n\n",
        "action_size = env.action_space.n\n",
        "agent = DQNAgent(state_size, action_size)#initialise the agent\n",
        "agent.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u1qLURhBWS4"
      },
      "source": [
        "Now plot the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BW1PlaP3nE59"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(total_rewards_log)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4QTZz7Ef_A7"
      },
      "source": [
        "We can see that DQN is learning . The longer you run it, the better and more stable it will get. Compare this performance with that of week 7's workshop. What do you notice? Why?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rl_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
