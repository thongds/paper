{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8705974",
   "metadata": {
    "id": "e8705974"
   },
   "source": [
    "# Adaptive Q-Learning in a 25×25 GridWorld (8 actions with probabilistic transitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fad8d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the utils directory to the Python path and import utils\n",
    "from utils import *\n",
    "from MLPQLearningAgent import *\n",
    "from OracleQLearningAgent import *\n",
    "from TransitionModelLearner import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23aefc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GridWorld environment with probabilistic transitions\n",
    "# Probabilistic parameters:\n",
    "# success_prob = 0.8: 80% chance of intended action\n",
    "# noise_prob = 0.1: 10% chance of random action  \n",
    "# stay_prob = 0.1: 10% chance of staying in place\n",
    "grid_world = GridWorld(\n",
    "    n_rows=25,\n",
    "    n_cols=25,\n",
    "    start=(0, 0),\n",
    "    goal=(24, 24),\n",
    "    walls = WALLS,\n",
    "    step_reward=-0.1,\n",
    "    goal_reward=1.0,\n",
    "    gamma=0.95,\n",
    "    success_prob=0.8,\n",
    "    noise_prob=0.1,\n",
    "    stay_prob=0.1\n",
    ")\n",
    "\n",
    "def oracle_model(s, action):\n",
    "    \"\"\"Oracle model that predicts the next state for diagonal actions (4-7)\"\"\"\n",
    "    s_model_next, r_model, done_model = grid_world.step(grid_world.from_index(s), action, ACTIONS_8)\n",
    "    return s_model_next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10ed662",
   "metadata": {},
   "source": [
    "### Agent algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc8387",
   "metadata": {},
   "source": [
    "## Probabilistic vs Deterministic Environment Comparison\n",
    "\n",
    "The environment now supports **probabilistic transitions** with the following parameters:\n",
    "- **Success probability (0.8)**: 80% chance the intended action is executed\n",
    "- **Noise probability (0.1)**: 10% chance a random action is executed instead  \n",
    "- **Stay probability (0.1)**: 10% chance the agent stays in the current position\n",
    "\n",
    "This makes the environment more realistic and challenging compared to the previous deterministic version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6KwB1VYqtufG",
   "metadata": {
    "id": "6KwB1VYqtufG"
   },
   "source": [
    "## **1. Q learning with 4 actions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2afa6a07",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2afa6a07",
    "outputId": "ca075cc9-d77b-4db6-ba57-c754533b5f0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (4 actions) complete!\n",
      "\n",
      "Policy Visualization:\n",
      " S  →  ↓  ▓  ↑  →  ↑  →  ↓  →  ↓  ←  →  →  ↑  ↑  ↓  →  ↓  ←  ↑  ↓  ←  →  ↓ \n",
      " ←  ↑  →  →  →  ↑  ↑  ↑  →  ▓  →  →  →  →  ↓  ↓  ↓  ↓  ←  ↑  ←  ←  ↓  ▓  → \n",
      " ↑  ↓  ▓  ↑  →  →  ←  →  →  ↓  ↓  →  ↑  →  ↓  ▓  →  ↓  ↓  ←  ←  ←  →  ↓  ← \n",
      " ▓  →  ↑  ↓  ▓  ↓  →  ↑  ↑  ↑  ↓  ←  →  →  ↓  →  ←  →  ↓  ↓  ←  →  ▓  ↑  ↓ \n",
      " ↓  ←  ↓  ↓  ↑  ↑  ▓  ←  →  ↓  →  →  ↑  ↓  ▓  ↓  →  ↑  ▓  ↑  ←  ↓  ↓  ↓  ← \n",
      " ↓  ▓  ↓  ←  ↓  ↑  ↓  ↓  →  ↓  ↓  ▓  ←  →  ↓  →  →  ↓  →  →  ↓  ↑  ↓  ↑  ↓ \n",
      " →  →  ←  →  ↓  ▓  ←  ▓  →  ↓  ↓  ↓  ▓  ←  ↓  ↑  →  →  ↓  ↓  →  ↑  ↓  ↓  ↓ \n",
      " →  ↑  ↑  ▓  ↓  ↓  ↓  ↑  ↓  ↑  ↓  ↓  ←  ↑  →  →  ▓  ←  ↓  ▓  →  ↓  →  →  ↓ \n",
      " ←  ↓  ▓  ↑  ↓  →  →  →  ↓  ←  ↓  ↑  →  →  →  ↑  ▓  ↓  ↑  ↓  ▓  →  ↓  ↓  ↓ \n",
      " →  ←  ↓  ←  ←  ↓  ←  ↑  ▓  ↑  →  ↓  →  ↑  ▓  →  →  →  ▓  ↓  ↓  ↓  ←  ↓  ↓ \n",
      " ←  ↓  ↑  ↑  ↑  ↓  →  →  →  ←  ▓  ↓  →  ↓  ↓  →  ↓  →  ↓  ↓  →  ↓  ▓  ↓  → \n",
      " →  →  →  ▓  →  →  →  →  →  ↑  ↓  ←  →  ↓  ↑  ▓  ↓  ↓  →  ↓  ↓  →  →  ↓  ← \n",
      " →  ↓  →  ↓  ←  →  →  →  ▓  →  ↑  ↑  →  →  ↓  →  ←  ↑  ▓  ←  ↓  →  ↓  ↓  ↓ \n",
      " ←  ↓  ↓  ↑  ←  ↓  ▓  ↓  ←  ↓  ↓  →  ←  →  ↓  →  ↓  ▓  ↓  ↓  →  ↓  ↓  →  ↓ \n",
      " ↓  ←  ↓  →  ←  ←  ←  ←  ↓  ←  →  →  ▓  →  ↓  →  ↓  ↓  ↓  ↓  ▓  ↓  ↓  →  ↓ \n",
      " ↓  →  ▓  ↓  ▓  ↑  ↑  →  ↓  ▓  ↓  →  ↓  →  →  →  →  ↓  ▓  ↓  ←  →  ↓  →  ↓ \n",
      " ↓  →  ↓  ↓  →  ↑  ↓  ▓  →  ↓  ↑  ▓  →  →  ↓  ▓  ↓  ↓  ↓  →  ↓  ↓  ↓  ↓  ↓ \n",
      " ↓  →  ←  ▓  ↓  ↓  ▓  ↓  ↓  →  ←  ↑  ←  →  →  →  →  →  →  →  ↓  ▓  ↓  ←  ↓ \n",
      " ↑  ↓  →  ↓  ↓  ▓  →  →  →  ↓  ↓  ↑  ↓  →  ▓  →  →  ↓  →  →  ↓  ↓  ↓  ▓  ↓ \n",
      " ←  →  ▓  ←  →  ↑  ↓  →  ▓  →  →  ↓  ↓  →  ↓  ↓  ▓  →  →  →  →  ↓  →  ↓  ↓ \n",
      " →  ▓  →  ←  →  →  →  ↑  ↑  →  ↑  ↓  ↓  ▓  →  ↓  ↓  ▓  ↓  →  →  ↓  →  ↓  ← \n",
      " →  ↓  ↓  ↑  ↓  →  ↓  ▓  →  →  →  →  →  →  ↑  →  ↓  ↑  ↓  ▓  ↓  ↓  ↓  ↓  ▓ \n",
      " →  →  ↓  →  ▓  ←  ↑  ↑  ←  ↓  ↓  →  ↓  →  ↓  ▓  →  →  →  ↓  →  ↓  ↓  →  ↓ \n",
      " ↓  ▓  →  →  ↓  ↑  →  ↓  ▓  ←  ↓  ←  →  →  →  →  ↓  ↑  →  →  →  →  →  ↓  ↓ \n",
      " ←  ↑  ↓  ↓  ↓  ↑  →  →  →  →  →  →  →  →  →  →  →  →  →  ↑  ↑  ▓  →  →  G \n",
      "\n",
      "Q-table shape: (625, 4)\n",
      "Value function (max Q-values):\n",
      "  -1.7   -1.6   -1.6    0.0   -1.5   -1.5   -1.5   -1.5   -1.5   -1.5   -1.4   -1.4   -1.4   -1.4   -1.3   -1.3   -1.3   -1.3   -1.3   -1.3   -1.3   -1.3   -1.3   -1.3   -1.3 \n",
      "  -1.6   -1.6   -1.6   -1.6   -1.6   -1.5   -1.5   -1.5   -1.5    0.0   -1.4   -1.4   -1.4   -1.4   -1.3   -1.3   -1.3   -1.3   -1.3   -1.3   -1.3   -1.3   -1.3    0.0   -1.2 \n",
      "  -1.6   -1.6    0.0   -1.5   -1.5   -1.5   -1.5   -1.5   -1.5   -1.4   -1.4   -1.4   -1.4   -1.4   -1.3    0.0   -1.3   -1.3   -1.2   -1.2   -1.2   -1.2   -1.2   -1.2   -1.2 \n",
      "   0.0   -1.6   -1.6   -1.6    0.0   -1.5   -1.5   -1.5   -1.4   -1.4   -1.4   -1.4   -1.4   -1.4   -1.3   -1.3   -1.3   -1.3   -1.2   -1.2   -1.2   -1.3    0.0   -1.2   -1.2 \n",
      "  -1.5   -1.6   -1.5   -1.5   -1.5   -1.5    0.0   -1.5   -1.4   -1.4   -1.4   -1.4   -1.4   -1.4    0.0   -1.3   -1.3   -1.3    0.0   -1.2   -1.2   -1.2   -1.2   -1.2   -1.2 \n",
      "  -1.5    0.0   -1.5   -1.5   -1.5   -1.5   -1.5   -1.4   -1.4   -1.4   -1.4    0.0   -1.4   -1.3   -1.3   -1.3   -1.3   -1.3   -1.3   -1.2   -1.2   -1.2   -1.2   -1.2   -1.2 \n",
      "  -1.5   -1.5   -1.5   -1.5   -1.5    0.0   -1.4    0.0   -1.4   -1.4   -1.4   -1.4    0.0   -1.3   -1.3   -1.3   -1.3   -1.3   -1.3   -1.2   -1.2   -1.2   -1.2   -1.2   -1.2 \n",
      "  -1.5   -1.5   -1.5    0.0   -1.5   -1.5   -1.4   -1.4   -1.4   -1.4   -1.3   -1.3   -1.3   -1.3   -1.3   -1.2    0.0   -1.2   -1.2    0.0   -1.2   -1.2   -1.2   -1.1   -1.1 \n",
      "  -1.5   -1.5    0.0   -1.4   -1.4   -1.4   -1.4   -1.4   -1.4   -1.4   -1.3   -1.3   -1.3   -1.3   -1.2   -1.2    0.0   -1.2   -1.2   -1.1    0.0   -1.2   -1.2   -1.1   -1.1 \n",
      "  -1.4   -1.4   -1.4   -1.4   -1.4   -1.4   -1.4   -1.4    0.0   -1.3   -1.3   -1.3   -1.3   -1.2    0.0   -1.2   -1.2   -1.2    0.0   -1.1   -1.1   -1.1   -1.1   -1.0   -1.1 \n",
      "  -1.4   -1.4   -1.4   -1.4   -1.4   -1.4   -1.4   -1.4   -1.3   -1.3    0.0   -1.3   -1.3   -1.2   -1.2   -1.2   -1.2   -1.2   -1.1   -1.1   -1.1   -1.0    0.0   -0.9   -1.0 \n",
      "  -1.4   -1.4   -1.4    0.0   -1.4   -1.4   -1.4   -1.4   -1.3   -1.3   -1.2   -1.3   -1.2   -1.2   -1.2    0.0   -1.1   -1.1   -1.1   -1.0   -1.0   -1.0   -0.9   -0.8   -1.0 \n",
      "  -1.4   -1.4   -1.4   -1.3   -1.4   -1.4   -1.3   -1.3    0.0   -1.3   -1.3   -1.2   -1.2   -1.2   -1.1   -1.1   -1.1   -1.1    0.0   -1.0   -0.9   -0.9   -0.9   -0.8   -0.8 \n",
      "  -1.4   -1.4   -1.3   -1.3   -1.4   -1.4    0.0   -1.3   -1.3   -1.3   -1.2   -1.2   -1.2   -1.2   -1.1   -1.1   -1.0    0.0   -0.9   -0.9   -0.8   -0.7   -0.7   -0.7   -0.6 \n",
      "  -1.4   -1.3   -1.3   -1.3   -1.3   -1.3   -1.3   -1.3   -1.3   -1.2   -1.2   -1.2    0.0   -1.1   -1.1   -1.0   -1.0   -0.9   -0.9   -0.8    0.0   -0.6   -0.6   -0.7   -0.5 \n",
      "  -1.3   -1.3    0.0   -1.3    0.0   -1.3   -1.3   -1.3   -1.2    0.0   -1.2   -1.2   -1.1   -1.1   -1.0   -0.9   -0.9   -0.8    0.0   -0.7   -0.7   -0.5   -0.4   -0.7   -0.4 \n",
      "  -1.3   -1.3   -1.3   -1.3   -1.3   -1.3   -1.3    0.0   -1.2   -1.2   -1.1    0.0   -1.1   -1.0   -0.9    0.0   -0.8   -0.7   -0.7   -0.6   -0.4   -0.7   -0.2   -0.6   -0.2 \n",
      "  -1.3   -1.3   -1.3    0.0   -1.3   -1.3    0.0   -1.2   -1.2   -1.1   -1.1   -1.1   -1.1   -1.0   -0.9   -0.8   -0.7   -0.7   -0.6   -0.5   -0.3    0.0   -0.1   -0.3   -0.1 \n",
      "  -1.3   -1.3   -1.3   -1.3   -1.2    0.0   -1.2   -1.1   -1.1   -1.1   -1.1   -1.1   -1.0   -1.0    0.0   -0.8   -0.7   -0.5   -0.6   -0.4   -0.3   -0.3   -0.1    0.0    0.0 \n",
      "  -1.3   -1.3    0.0   -1.2   -1.2   -1.2   -1.2   -1.1    0.0   -1.1   -1.1   -1.1   -1.0   -0.9   -0.8   -0.8    0.0   -0.4   -0.5   -0.3   -0.2   -0.1    0.0    0.2    0.1 \n",
      "  -1.3    0.0   -1.2   -1.2   -1.2   -1.2   -1.1   -1.1   -1.1   -1.1   -1.0   -1.0   -1.0    0.0   -0.6   -0.5   -0.6    0.0   -0.5   -0.4   -0.1    0.1    0.1    0.3    0.2 \n",
      "  -1.3   -1.2   -1.2   -1.2   -1.2   -1.2   -1.1    0.0   -1.1   -1.1   -1.0   -1.0   -0.9   -0.8   -0.7   -0.4   -0.3   -0.5   -0.5    0.0   -0.0    0.3    0.4    0.5    0.0 \n",
      "  -1.2   -1.2   -1.2   -1.2    0.0   -1.1   -1.1   -1.1   -1.1   -1.1   -1.0   -1.0   -0.9   -0.8   -0.6    0.0   -0.2   -0.1    0.0    0.1    0.1    0.3    0.5    0.7    0.7 \n",
      "  -1.2    0.0   -1.2   -1.2   -1.2   -1.1   -1.1   -1.1    0.0   -1.0   -1.0   -0.9   -0.8   -0.6   -0.6   -0.5   -0.4   -0.4   -0.0    0.2    0.3    0.5    0.6    0.8    0.9 \n",
      "  -1.2   -1.2   -1.2   -1.2   -1.1   -1.1   -1.1   -1.1   -1.0   -1.0   -0.9   -0.8   -0.7   -0.6   -0.4   -0.3   -0.2   -0.1    0.0    0.1    0.0    0.0    0.8    1.0    0.0 \n"
     ]
    }
   ],
   "source": [
    "# Q-learning with 4 actions using refactored approach\n",
    "rng = np.random.default_rng(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "NUM_STATES = N_ROWS * N_COLS\n",
    "NUM_ACTIONS_4 = 4\n",
    "Q4 = np.zeros((NUM_STATES, NUM_ACTIONS_4), dtype=float)\n",
    "\n",
    "returns = np.zeros(EPISODES, dtype=float)\n",
    "bumps = np.zeros(EPISODES, dtype=float)\n",
    "steps_arr = np.zeros(EPISODES, dtype=int)\n",
    "\n",
    "eps = EPS_START\n",
    "eps_decay = (EPS_START - EPS_END) / max(1, EPS_DECAY_EPISODES)\n",
    "\n",
    "for ep in range(EPISODES):\n",
    "    s = START\n",
    "    si = grid_world.to_index(s)\n",
    "    done = False\n",
    "    G = 0.0\n",
    "    bumpcount = 0\n",
    "    disc = 1.0\n",
    "    steps = 0\n",
    "\n",
    "    for t in range(MAX_STEPS):\n",
    "        a = epsilon_greedy(Q4[si], eps, rng)\n",
    "        s_next, r, done = grid_world.step(s, a, ACTIONS_4, rng)\n",
    "        s_next_i = grid_world.to_index(s_next)\n",
    "\n",
    "        if si == s_next_i:\n",
    "            bumpcount += 1\n",
    "            r = BUMP_REWARD\n",
    "\n",
    "        target = r if done else r + GAMMA * np.max(Q4[s_next_i])\n",
    "        Q4[si, a] += ALPHA * (target - Q4[si, a])\n",
    "\n",
    "        G += r\n",
    "        disc *= GAMMA\n",
    "        s, si = s_next, s_next_i\n",
    "        steps += 1\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    returns[ep] = G\n",
    "    bumps[ep] = bumpcount\n",
    "    steps_arr[ep] = steps\n",
    "    if ep < EPS_DECAY_EPISODES:\n",
    "        eps = max(EPS_END, eps - eps_decay)\n",
    "\n",
    "print('Training (4 actions) complete!')\n",
    "returns4 = returns.copy()\n",
    "bumps4 = bumps.copy()\n",
    "ma_w = 25\n",
    "ret_ma4 = moving_average(returns4, w=ma_w)\n",
    "\n",
    "policy4 = Visualizer.derive_policy(Q4, N_ROWS, N_COLS)\n",
    "Visualizer.render_policy(policy4, N_ROWS, N_COLS, WALLS, START, GOAL)\n",
    "Visualizer.print_value_grid(Q4, N_ROWS, N_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5f425a",
   "metadata": {
    "id": "3e5f425a"
   },
   "source": [
    "## Add 4 diagonal actions: South-East, South-West, North-East, North-West\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "866c58d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "866c58d2",
    "outputId": "754f7310-47d9-4904-f648-5f4de67d8325"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing diagonal actions from position (3, 2):\n",
      "Action 4 SE (South-East): (3, 2) -> (4, 3) | reward=-0.1 | done=False\n",
      "Action 5 SW (South-West): (3, 2) -> (3, 2) | reward=-0.1 | done=False\n",
      "Action 6 NE (North-East): (3, 2) -> (2, 3) | reward=-0.1 | done=False\n",
      "Action 7 NW (North-West): (3, 2) -> (2, 1) | reward=-0.1 | done=False\n",
      "\n",
      "Current policy (with 4 actions only):\n",
      "\n",
      "Policy Visualization:\n",
      " S  →  ↓  ▓  ↑  →  ↑  →  ↓  →  ↓  ←  →  →  ↑  ↑  ↓  →  ↓  ←  ↑  ↓  ←  →  ↓ \n",
      " ←  ↑  →  →  →  ↑  ↑  ↑  →  ▓  →  →  →  →  ↓  ↓  ↓  ↓  ←  ↑  ←  ←  ↓  ▓  → \n",
      " ↑  ↓  ▓  ↑  →  →  ←  →  →  ↓  ↓  →  ↑  →  ↓  ▓  →  ↓  ↓  ←  ←  ←  →  ↓  ← \n",
      " ▓  →  ↑  ↓  ▓  ↓  →  ↑  ↑  ↑  ↓  ←  →  →  ↓  →  ←  →  ↓  ↓  ←  →  ▓  ↑  ↓ \n",
      " ↓  ←  ↓  ↓  ↑  ↑  ▓  ←  →  ↓  →  →  ↑  ↓  ▓  ↓  →  ↑  ▓  ↑  ←  ↓  ↓  ↓  ← \n",
      " ↓  ▓  ↓  ←  ↓  ↑  ↓  ↓  →  ↓  ↓  ▓  ←  →  ↓  →  →  ↓  →  →  ↓  ↑  ↓  ↑  ↓ \n",
      " →  →  ←  →  ↓  ▓  ←  ▓  →  ↓  ↓  ↓  ▓  ←  ↓  ↑  →  →  ↓  ↓  →  ↑  ↓  ↓  ↓ \n",
      " →  ↑  ↑  ▓  ↓  ↓  ↓  ↑  ↓  ↑  ↓  ↓  ←  ↑  →  →  ▓  ←  ↓  ▓  →  ↓  →  →  ↓ \n",
      " ←  ↓  ▓  ↑  ↓  →  →  →  ↓  ←  ↓  ↑  →  →  →  ↑  ▓  ↓  ↑  ↓  ▓  →  ↓  ↓  ↓ \n",
      " →  ←  ↓  ←  ←  ↓  ←  ↑  ▓  ↑  →  ↓  →  ↑  ▓  →  →  →  ▓  ↓  ↓  ↓  ←  ↓  ↓ \n",
      " ←  ↓  ↑  ↑  ↑  ↓  →  →  →  ←  ▓  ↓  →  ↓  ↓  →  ↓  →  ↓  ↓  →  ↓  ▓  ↓  → \n",
      " →  →  →  ▓  →  →  →  →  →  ↑  ↓  ←  →  ↓  ↑  ▓  ↓  ↓  →  ↓  ↓  →  →  ↓  ← \n",
      " →  ↓  →  ↓  ←  →  →  →  ▓  →  ↑  ↑  →  →  ↓  →  ←  ↑  ▓  ←  ↓  →  ↓  ↓  ↓ \n",
      " ←  ↓  ↓  ↑  ←  ↓  ▓  ↓  ←  ↓  ↓  →  ←  →  ↓  →  ↓  ▓  ↓  ↓  →  ↓  ↓  →  ↓ \n",
      " ↓  ←  ↓  →  ←  ←  ←  ←  ↓  ←  →  →  ▓  →  ↓  →  ↓  ↓  ↓  ↓  ▓  ↓  ↓  →  ↓ \n",
      " ↓  →  ▓  ↓  ▓  ↑  ↑  →  ↓  ▓  ↓  →  ↓  →  →  →  →  ↓  ▓  ↓  ←  →  ↓  →  ↓ \n",
      " ↓  →  ↓  ↓  →  ↑  ↓  ▓  →  ↓  ↑  ▓  →  →  ↓  ▓  ↓  ↓  ↓  →  ↓  ↓  ↓  ↓  ↓ \n",
      " ↓  →  ←  ▓  ↓  ↓  ▓  ↓  ↓  →  ←  ↑  ←  →  →  →  →  →  →  →  ↓  ▓  ↓  ←  ↓ \n",
      " ↑  ↓  →  ↓  ↓  ▓  →  →  →  ↓  ↓  ↑  ↓  →  ▓  →  →  ↓  →  →  ↓  ↓  ↓  ▓  ↓ \n",
      " ←  →  ▓  ←  →  ↑  ↓  →  ▓  →  →  ↓  ↓  →  ↓  ↓  ▓  →  →  →  →  ↓  →  ↓  ↓ \n",
      " →  ▓  →  ←  →  →  →  ↑  ↑  →  ↑  ↓  ↓  ▓  →  ↓  ↓  ▓  ↓  →  →  ↓  →  ↓  ← \n",
      " →  ↓  ↓  ↑  ↓  →  ↓  ▓  →  →  →  →  →  →  ↑  →  ↓  ↑  ↓  ▓  ↓  ↓  ↓  ↓  ▓ \n",
      " →  →  ↓  →  ▓  ←  ↑  ↑  ←  ↓  ↓  →  ↓  →  ↓  ▓  →  →  →  ↓  →  ↓  ↓  →  ↓ \n",
      " ↓  ▓  →  →  ↓  ↑  →  ↓  ▓  ←  ↓  ←  →  →  →  →  ↓  ↑  →  →  →  →  →  ↓  ↓ \n",
      " ←  ↑  ↓  ↓  ↓  ↑  →  →  →  →  →  →  →  →  →  →  →  →  →  ↑  ↑  ▓  →  →  G \n"
     ]
    }
   ],
   "source": [
    "# Demo: test all diagonal actions from (3,2) using the new action map\n",
    "s_demo = (3, 2)\n",
    "diagonal_actions = {\n",
    "    4: 'SE (South-East)',\n",
    "    5: 'SW (South-West)', \n",
    "    6: 'NE (North-East)',\n",
    "    7: 'NW (North-West)'\n",
    "}\n",
    "\n",
    "print(f\"Testing diagonal actions from position {s_demo}:\")\n",
    "for action, description in diagonal_actions.items():\n",
    "    s_next_demo, r_demo, done_demo = grid_world.step(s_demo, action, ACTIONS_8)\n",
    "    print(f'Action {action} {description}: {s_demo} -> {s_next_demo} | reward={r_demo} | done={done_demo}')\n",
    "\n",
    "print(\"\\nCurrent policy (with 4 actions only):\")\n",
    "Visualizer.render_policy(policy4, N_ROWS, N_COLS, WALLS, START, GOAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kqKAv3rNZ9uI",
   "metadata": {
    "id": "kqKAv3rNZ9uI"
   },
   "source": [
    "## **3. Q learning with 8 actions and an oracle model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xvTQtRszJIwM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xvTQtRszJIwM",
    "outputId": "41a13970-6d31-4fc6-f47c-4b6b995efdea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running multiple experiments for statistical analysis...\n",
      "Running experiment 1/10...\n",
      "Training (4 actions) complete!\n",
      "Training (8 actions) complete!\n",
      "Running experiment 2/10...\n",
      "Training (4 actions) complete!\n",
      "Training (8 actions) complete!\n",
      "Running experiment 3/10...\n",
      "Training (4 actions) complete!\n",
      "Training (8 actions) complete!\n",
      "Running experiment 4/10...\n",
      "Training (4 actions) complete!\n",
      "Training (8 actions) complete!\n",
      "Running experiment 5/10...\n",
      "Training (4 actions) complete!\n",
      "Training (8 actions) complete!\n",
      "Running experiment 6/10...\n",
      "Training (4 actions) complete!\n",
      "Training (8 actions) complete!\n",
      "Running experiment 7/10...\n",
      "Training (4 actions) complete!\n",
      "Training (8 actions) complete!\n",
      "Running experiment 8/10...\n",
      "Training (4 actions) complete!\n",
      "Training (8 actions) complete!\n"
     ]
    }
   ],
   "source": [
    "def run_multiple_experiments(n_runs=5, base_seed=123):\n",
    "    \"\"\"Run multiple experiments with different seeds and return statistics\"\"\"\n",
    "    \n",
    "    # Storage for all runs\n",
    "    all_returns4 = []\n",
    "    all_returns5plain = []\n",
    "    all_returns5oracle = []\n",
    "    all_returns5mlp = []\n",
    "    all_returns5ignore = []\n",
    "    all_bumps4 = []\n",
    "    all_bumps5plain = []\n",
    "    all_bumps5oracle = []\n",
    "    all_bumps5mlp = []\n",
    "    all_bumps5ignore = []\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        current_seed = base_seed + run * 42\n",
    "        # Reset random seeds\n",
    "        rng = np.random.default_rng(current_seed)\n",
    "        random.seed(current_seed)\n",
    "        np.random.seed(current_seed)\n",
    "        torch.manual_seed(current_seed)\n",
    "        \n",
    "        print(f\"Running experiment {run + 1}/{n_runs}...\")\n",
    "        \n",
    "        # 1. Q-learning with 4 actions\n",
    "        agent_plain_4_action = QLearningAgent(grid_world, 4, seed=current_seed)\n",
    "        agent_plain_4_action.train(ACTIONS_4, epsilon_greedy, BUMP_REWARD)\n",
    "        results_plain = agent_plain_4_action.get_results(moving_average)\n",
    "        all_returns4.append(results_plain['returns'])\n",
    "        all_bumps4.append(results_plain['bumps'])\n",
    "        \n",
    "        # 2. Plain Q-learning with 8 actions\n",
    "        agent_plain = QLearningAgent(grid_world, 8, seed=current_seed)\n",
    "        agent_plain.train(ACTIONS_8, epsilon_greedy, BUMP_REWARD)\n",
    "        results_plain = agent_plain.get_results(moving_average)\n",
    "        all_returns5plain.append(results_plain['returns'])\n",
    "        all_bumps5plain.append(results_plain['bumps'])\n",
    "        \n",
    "        # 3. Ignore model (only Q-table reuse)\n",
    "        ignore_model_run = OracleQLearningAgent(grid_world, 8, base_q_table=agent_plain_4_action.Q, seed=current_seed, use_model=False)\n",
    "        ignore_model_run.train_with_oracle(ACTIONS_8, epsilon_greedy, BUMP_REWARD, oracle_model, agent_plain_4_action.Q)\n",
    "        ignore_model_result_run = ignore_model_run.get_results(moving_average)\n",
    "        all_returns5ignore.append(ignore_model_result_run['returns'])\n",
    "        all_bumps5ignore.append(ignore_model_result_run['bumps'])\n",
    "        \n",
    "        # 4. Oracle Q-learning with 8 actions \n",
    "        oracle_agent_run = OracleQLearningAgent(grid_world, 8, base_q_table=agent_plain_4_action.Q, seed=current_seed)\n",
    "        oracle_agent_run.train_with_oracle(ACTIONS_8, epsilon_greedy, BUMP_REWARD, oracle_model, agent_plain_4_action.Q)\n",
    "        oracle_results_run = oracle_agent_run.get_results(moving_average)\n",
    "        all_returns5oracle.append(oracle_results_run['returns'])\n",
    "        all_bumps5oracle.append(oracle_results_run['bumps'])\n",
    "        \n",
    "        # 5. NEW: MLP Q-learning with 8 actions (our approach)\n",
    "        mlp_agent_run = MLPQLearningAgent(grid_world, 8, base_q_table=agent_plain_4_action.Q, seed=current_seed)\n",
    "        mlp_agent_run.train_with_learned_model(ACTIONS_8, epsilon_greedy, BUMP_REWARD, agent_plain_4_action.Q)\n",
    "        mlp_results_run = mlp_agent_run.get_results(moving_average)\n",
    "        all_returns5mlp.append(mlp_results_run['returns'])\n",
    "        all_bumps5mlp.append(mlp_results_run['bumps'])\n",
    "    \n",
    "    # Calculate statistics\n",
    "    all_returns4 = np.array(all_returns4)\n",
    "    all_returns5plain = np.array(all_returns5plain)\n",
    "    all_returns5oracle = np.array(all_returns5oracle)\n",
    "    all_returns5mlp = np.array(all_returns5mlp)\n",
    "    all_returns5ignore = np.array(all_returns5ignore)\n",
    "    all_bumps4 = np.array(all_bumps4)\n",
    "    all_bumps5plain = np.array(all_bumps5plain)\n",
    "    all_bumps5oracle = np.array(all_bumps5oracle)\n",
    "    all_bumps5mlp = np.array(all_bumps5mlp)\n",
    "    all_bumps5ignore = np.array(all_bumps5ignore)\n",
    "    \n",
    "    # Calculate moving averages for all runs\n",
    "    ma_w = 25\n",
    "    all_ret_ma4 = np.array([moving_average(returns, w=ma_w) for returns in all_returns4])\n",
    "    all_ret_ma5plain = np.array([moving_average(returns, w=ma_w) for returns in all_returns5plain])\n",
    "    all_ret_ma5oracle = np.array([moving_average(returns, w=ma_w) for returns in all_returns5oracle])\n",
    "    all_ret_ma5mlp = np.array([moving_average(returns, w=ma_w) for returns in all_returns5mlp])\n",
    "    all_ret_ma5ignore = np.array([moving_average(returns, w=ma_w) for returns in all_returns5ignore])\n",
    "    \n",
    "    all_bumps4_avg = np.array([moving_average(bumps, w=ma_w) for bumps in all_bumps4])\n",
    "    all_bumps5plain_avg = np.array([moving_average(bumps, w=ma_w) for bumps in all_bumps5plain])\n",
    "    all_bumps5oracle_avg = np.array([moving_average(bumps, w=ma_w) for bumps in all_bumps5oracle])\n",
    "    all_bumps5mlp_avg = np.array([moving_average(bumps, w=ma_w) for bumps in all_bumps5mlp])\n",
    "    all_bumps5ignore_avg = np.array([moving_average(bumps, w=ma_w) for bumps in all_bumps5ignore])\n",
    "    \n",
    "    return {\n",
    "        'returns': {\n",
    "            '4_actions': {'mean': np.mean(all_ret_ma4, axis=0), 'std': np.std(all_ret_ma4, axis=0)},\n",
    "            '5_plain': {'mean': np.mean(all_ret_ma5plain, axis=0), 'std': np.std(all_ret_ma5plain, axis=0)},\n",
    "            '5_oracle': {'mean': np.mean(all_ret_ma5oracle, axis=0), 'std': np.std(all_ret_ma5oracle, axis=0)},\n",
    "            '5_mlp': {'mean': np.mean(all_ret_ma5mlp, axis=0), 'std': np.std(all_ret_ma5mlp, axis=0)},\n",
    "            '5_ignore': {'mean': np.mean(all_ret_ma5ignore, axis=0), 'std': np.std(all_ret_ma5ignore, axis=0)}\n",
    "        },\n",
    "        'bumps': {\n",
    "            '4_actions': {'mean': np.mean(all_bumps4_avg, axis=0), 'std': np.std(all_bumps4_avg, axis=0)},\n",
    "            '5_plain': {'mean': np.mean(all_bumps5plain_avg, axis=0), 'std': np.std(all_bumps5plain_avg, axis=0)},\n",
    "            '5_oracle': {'mean': np.mean(all_bumps5oracle_avg, axis=0), 'std': np.std(all_bumps5oracle_avg, axis=0)},\n",
    "            '5_mlp': {'mean': np.mean(all_bumps5mlp_avg, axis=0), 'std': np.std(all_bumps5mlp_avg, axis=0)},\n",
    "            '5_ignore': {'mean': np.mean(all_bumps5ignore_avg, axis=0), 'std': np.std(all_bumps5ignore_avg, axis=0)}\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Run multiple experiments\n",
    "print(\"Running multiple experiments for statistical analysis...\")\n",
    "stats = run_multiple_experiments(n_runs=10)  # Reduced runs for faster execution\n",
    "\n",
    "# Plot results with shaded error bars\n",
    "def plot_with_shaded_errors(stats, figsize=(12, 6)):\n",
    "    \"\"\"Plot results with shaded error bars\"\"\"\n",
    "    \n",
    "    # Returns plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Calculate x-axis for each series (they might have different lengths due to moving average)\n",
    "    episodes_total = EPISODES\n",
    "    ma_w = 25\n",
    "    \n",
    "    # Plot each method with shaded error bars\n",
    "    methods = [\n",
    "        ('4_actions', 'Q learning (4 actions)', 'blue'),\n",
    "        ('5_plain', 'Q learning (8 actions)', 'black'), \n",
    "        ('5_oracle', 'Oracle-based adaptation (perfect model)', 'red'),\n",
    "        ('5_mlp', 'MLP-based adaptation (learned model)', 'orange'),\n",
    "        ('5_ignore', 'Q-table reuse only', 'green')\n",
    "    ]\n",
    "    \n",
    "    for method_key, label, color in methods:\n",
    "        mean_vals = stats['returns'][method_key]['mean']\n",
    "        std_vals = stats['returns'][method_key]['std']\n",
    "        x_vals = np.arange(episodes_total - len(mean_vals), episodes_total)\n",
    "        \n",
    "        # Plot mean line\n",
    "        plt.plot(x_vals, mean_vals, color=color, label=f'Return: {label}', linewidth=2)\n",
    "        \n",
    "        # Plot shaded error region (mean ± std)\n",
    "        plt.fill_between(x_vals, mean_vals - std_vals, mean_vals + std_vals, \n",
    "                        color=color, alpha=0.2)\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Discounted return')\n",
    "    plt.title('Q-learning in GridWorld: Comparison of Adaptation Methods')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Bumps plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    for method_key, label, color in methods:\n",
    "        mean_vals = stats['bumps'][method_key]['mean']\n",
    "        std_vals = stats['bumps'][method_key]['std']\n",
    "        x_vals = np.arange(len(mean_vals))\n",
    "        \n",
    "        # Plot mean line\n",
    "        plt.plot(x_vals, mean_vals, color=color, label=f'Total Bumps: {label}', linewidth=2)\n",
    "        \n",
    "        # Plot shaded error region (mean ± std)\n",
    "        plt.fill_between(x_vals, mean_vals - std_vals, mean_vals + std_vals, \n",
    "                        color=color, alpha=0.2)\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Bumps')\n",
    "    plt.title('Q-learning in GridWorld: Collision Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the results with shaded error bars\n",
    "plot_with_shaded_errors(stats)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
